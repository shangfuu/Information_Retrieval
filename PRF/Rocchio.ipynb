{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR - HW5 Query Modeling\n",
    "- In this project, you will have\n",
    "    - 150 Queries\n",
    "        - 60%PublicQueries&40%PrivateQueries\n",
    "    - 30,000 Documents\n",
    "- Our goal is to implement a PRF algorithm for retrieval\n",
    "        \n",
    "https://www.kaggle.com/c/2020-information-retrieval-and-applications-hw5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'ntust-ir-2020_hw5_new'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 80\n",
    "# BM25\n",
    "k3 = 100\n",
    "b = 0.75\n",
    "k1 = 0.8\n",
    "\n",
    "# Rocchio\n",
    "n_rel = 0\n",
    "rel = 10\n",
    "iter_num = 3\n",
    "ALPHA = 1\n",
    "BETA = 0.75\n",
    "GAMMA = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_files(root_path = DATA_PATH, extension = \".txt\"):\n",
    "    \"\"\"\n",
    "    Retrun terms in querys and docs, the list of querys and docs name.\n",
    "    \"\"\"\n",
    "    \n",
    "    path_query = DATA_PATH + \"/queries\"\n",
    "    path_docs = DATA_PATH + \"/docs\"\n",
    "    \n",
    "    qlf = open(os.path.join(DATA_PATH, \"query_list.txt\"))\n",
    "    dlf = open(os.path.join(DATA_PATH, \"doc_list.txt\"))\n",
    "    \n",
    "    querys = {}\n",
    "    query_name = []\n",
    "    for fname in qlf:\n",
    "        fname = fname.strip(\"\\n\")\n",
    "        file = os.path.join(path_query, fname + extension)\n",
    "        \n",
    "        fq = open(file)\n",
    "        query = [q.strip('\\n').lower().split(' ') for q in fq][0]\n",
    "        querys[fname] = query\n",
    "        query_name.append(fname)\n",
    "        fq.close()\n",
    "\n",
    "    docs = {}\n",
    "    doc_name = []\n",
    "    for fname in dlf:\n",
    "        fname = fname.strip(\"\\n\")\n",
    "        file = os.path.join(path_docs, fname + extension)\n",
    "        \n",
    "        fd = open(file)\n",
    "        doc = [d.strip(\"\\n\").lower().split(\" \") for d in fd][0]\n",
    "#         docs.append(doc)\n",
    "        docs[fname] = doc\n",
    "        doc_name.append(fname)\n",
    "        fd.close()\n",
    "\n",
    "    dlf.close()\n",
    "    qlf.close()\n",
    "    \n",
    "    \n",
    "    return querys, docs, query_name, doc_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF(WV, vec_dict, word_2_id, subLinear = False):\n",
    "    \"\"\"\n",
    "    Return TF.\n",
    "    \"\"\"\n",
    "    \n",
    "    _tf = {}\n",
    "\n",
    "    for fname in vec_dict.keys():\n",
    "        _tf[fname] = np.ones(len(WV))\n",
    "    \n",
    "    for fname, terms in vec_dict.items():\n",
    "        for term in terms:\n",
    "            if term in word_2_id:\n",
    "                _tf[fname][word_2_id[term]] += 1\n",
    "    \n",
    "    if subLinear:\n",
    "        # sublinear_tf: replace tf with 1 + log(tf).\n",
    "        for fname, tf_val in _tf.items():\n",
    "            _tf[fname] = 1 + np.log(_tf[fname])\n",
    "#             for idx, val in enumerate(tf_val):\n",
    "#                 try:\n",
    "#                     _tf[fname][idx] = 1 + math.log(val) \n",
    "#                 except:\n",
    "#                     print(f'error tf: {val}')\n",
    "\n",
    "    return _tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDF(WV, vec, word_2_id, subLinear=False):\n",
    "    \"\"\"\n",
    "    Return IDF.\n",
    "    \"\"\"\n",
    "    ni = np.zeros(len(WV))\n",
    "    \n",
    "    # TF\n",
    "    _tf = TF(WV, vec, word_2_id, subLinear)\n",
    "    \n",
    "    print(\"[INFO] TF Done...\")\n",
    "    \n",
    "    for terms in vec.values():\n",
    "        check_term = {}\n",
    "        for term in terms:\n",
    "            if term in word_2_id and term not in check_term:\n",
    "                ni[word_2_id[term]] += 1\n",
    "                check_term[term] = True\n",
    "    # IDF\n",
    "    _idf = ni.copy()\n",
    "    for idx, n in enumerate(ni):\n",
    "        _idf[idx] = math.log( (len(vec) - n + 0.5) / (0.5 + n) )\n",
    "#         _idf[idx] = math.log( ((1 + len(vec)) / (1 + n)) + 1) \n",
    "    \n",
    "    print(\"[INFO] IDF Done...\")\n",
    "    \n",
    "    # TF-IDF\n",
    "    tf_idf = {}\n",
    "    for fname, terms in vec.items():\n",
    "        tf_idf[fname] = np.zeros(len(WV))\n",
    "        for term in terms:\n",
    "            if term in word_2_id:\n",
    "                tf_idf[fname][word_2_id[term]] = _tf[fname][word_2_id[term]] * _idf[word_2_id[term]]\n",
    "\n",
    "    print(\"[INFO] TF-IDF Done...\")\n",
    "    \n",
    "    return tf_idf, _tf, _idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgDocLength(docs):\n",
    "    \"\"\"\n",
    "    Average documents length.\n",
    "    \"\"\"\n",
    "    avgDL = 0\n",
    "    for term in docs.values():\n",
    "        avgDL += len(term)\n",
    "    return avgDL / len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(word_vec):\n",
    "    \"\"\"\n",
    "    Make the Word Vector.\n",
    "    \"\"\"\n",
    "    word_terms = set()\n",
    "    for terms in word_vec.values():\n",
    "        for t in terms:\n",
    "            word_terms.add(t)\n",
    "    word_terms = list(word_terms)\n",
    "    \n",
    "    word_2_id = {}\n",
    "    for idx, word in enumerate(word_terms):\n",
    "        word_2_id[word] = idx\n",
    "    \n",
    "    return word_terms, word_2_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector_filter(word_vec, querys, filter_type=1, tfidf_thresh = 120):\n",
    "\n",
    "    # query terms\n",
    "    if filter_type == 1:\n",
    "        WV, word_2_id = word_vector(word_vec)\n",
    "    # query terms and some high tf terms.\n",
    "    elif filter_type == 2:\n",
    "        WV, word_2_id = word_vector(word_vec)\n",
    "#         tf = TF(WV, word_vec, word_2_id)\n",
    "        tfidf, tf, idf = TFIDF(WV, word_vec, word_2_id)\n",
    "        \n",
    "        # query terms\n",
    "        use_terms = set()\n",
    "        WV_q, _ = word_vector(querys)\n",
    "        # update WV\n",
    "        for fname, words in word_vec.items():\n",
    "            for word in words:\n",
    "                if word in WV_q:\n",
    "                    use_terms.add(word)\n",
    "                elif len(word) > 1 and not word.isdigit() and tfidf[fname][word_2_id[word]] > tfidf_thresh:\n",
    "            #            print(f'word: {word}, tfidf: {tfidf[fname][word_2_id[word]]}')\n",
    "                    use_terms.add(word)\n",
    "        WV = list(use_terms)\n",
    "\n",
    "        # update word_2_id\n",
    "        word_2_id = {}\n",
    "        for idx, word in enumerate(WV):\n",
    "            word_2_id[word] = idx\n",
    "\n",
    "    return WV, word_2_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "querys, docs, query_name, doc_name = open_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vec = docs.copy()\n",
    "all_vec.update(querys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] TF Done...\n",
      "[INFO] IDF Done...\n",
      "[INFO] TF-IDF Done...\n"
     ]
    }
   ],
   "source": [
    "WV, word_2_id = word_vector_filter(all_vec, querys, filter_type=2, tfidf_thresh=thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WV > 50000 runs tfidf need almost 20 min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10325"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(WV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] TF Done...\n",
      "[INFO] IDF Done...\n",
      "[INFO] TF-IDF Done...\n"
     ]
    }
   ],
   "source": [
    "tfidf, tf, idf = TFIDF(WV, all_vec, word_2_id, subLinear=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "query_vecs = {k:v/np.linalg.norm(v) for k, v in tfidf.items() if k in querys}\n",
    "# query_vecs = {k:v for k, v in tfidf.items() if k in querys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251.56637811660767"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(vec1, vec2):\n",
    "    dot = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    try:\n",
    "        cos = dot / (norm1 * norm2)\n",
    "    except:\n",
    "        cos = 0\n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ranking(query_vec):\n",
    "    avgDL = avgDocLength(docs)\n",
    "\n",
    "    sim_dict = {}\n",
    "    sorted_sim_dict = {}\n",
    "    i = 0\n",
    "    for fqname, terms_q in querys.items():\n",
    "        print(i)\n",
    "        i += 1\n",
    "\n",
    "        sim_dict[fqname] = {}\n",
    "        for fdname, terms_d in docs.items():\n",
    "            score = 0.0\n",
    "            for term in terms_q:\n",
    "                if term not in terms_d:\n",
    "                    continue\n",
    "                term_id = word_2_id[term]\n",
    "                score += (k1 + 1) * tf[fdname][term_id] / (k1 * ((1 - b) + b * len(terms_d) / avgDL) + tf[fdname][term_id]) * idf[term_id]\n",
    "            sim_dict[fqname][fdname] = cosine(query_vecs[fqname], tfidf[fdname]) * score\n",
    "        sorted_sim_dict[fqname] = sorted(sim_dict[fqname], key = sim_dict[fqname].get, reverse=True)\n",
    "    return sorted_sim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_ranking(query_vec):\n",
    "    \"\"\"\n",
    "    Return the score of relevant document ranked by using BM25 and VSM, and the BM25 rank.\n",
    "    \"\"\"\n",
    "    avgDL = avgDocLength(docs)\n",
    "\n",
    "    sim_dict = {}\n",
    "    sorted_sim_dict = {}\n",
    "    BM_rank = {}\n",
    "    i = 0\n",
    "    for fqname, terms_q in querys.items():\n",
    "        print(i)\n",
    "        i += 1\n",
    "\n",
    "        sim_dict[fqname] = {}\n",
    "        BM_rank[fqname] = {}\n",
    "        for fdname, terms_d in docs.items():\n",
    "            BM_rank[fqname][fdname] = 0.0\n",
    "            for term in terms_q:\n",
    "                if term not in terms_d:\n",
    "                    continue\n",
    "                term_id = word_2_id[term]\n",
    "                BM_rank[fqname][fdname] += (k1 + 1) * tf[fdname][term_id] / (k1 * ((1 - b) + b * len(terms_d) / avgDL) + tf[fdname][term_id]) * idf[term_id]\n",
    "            sim_dict[fqname][fdname] = cosine(query_vecs[fqname], tfidf[fdname]) * BM_rank[fqname][fdname]\n",
    "        sorted_sim_dict[fqname] = sorted(sim_dict[fqname], key = sim_dict[fqname].get, reverse=True)\n",
    "    return sorted_sim_dict, BM_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking(query_vec):\n",
    "    \"\"\"\n",
    "    Used with initial one.\n",
    "    \"\"\"\n",
    "    sim_dict = {}\n",
    "    sorted_sim_dict = {}\n",
    "    i = 0\n",
    "    for fqname, terms_q in querys.items():\n",
    "        print(i)\n",
    "        i += 1\n",
    "\n",
    "        sim_dict[fqname] = {}\n",
    "        for fdname, terms_d in docs.items():\n",
    "            sim_dict[fqname][fdname] = cosine(query_vecs[fqname], tfidf[fdname]) * BM_rank[fqname][fdname]\n",
    "        sorted_sim_dict[fqname] = sorted(sim_dict[fqname], key = sim_dict[fqname].get, reverse=True)\n",
    "    return sorted_sim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n"
     ]
    }
   ],
   "source": [
    "VSM_rank, BM_rank = init_ranking(query_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n"
     ]
    }
   ],
   "source": [
    "for _ in range(iter_num):\n",
    "    # Rocchio\n",
    "    for q_name, rank_doc in VSM_rank.items():\n",
    "        mean_rel = []\n",
    "        mean_nrel = []\n",
    "        \n",
    "        for i in range(0, rel):\n",
    "            mean_rel.append(tfidf[rank_doc[i]] * (1 + (rel - i)) / rel)\n",
    "#             mean_rel.append(tfidf[rank_doc[i]])\n",
    "        for i in range(n_rel + len(rank_doc), len(rank_doc)):\n",
    "            mean_nrel.append(tfidf[rank_doc[i]])\n",
    "        \n",
    "        mean_rel = np.mean(mean_rel, axis=0)\n",
    "\n",
    "        if n_rel >= 0:\n",
    "            query_vecs[q_name] = ALPHA * query_vecs[q_name] + BETA * mean_rel            \n",
    "        else:            \n",
    "            mean_nrel = np.mean(mean_nrel, axis=0)\n",
    "            query_vecs[q_name] = ALPHA * query_vecs[q_name] + BETA * mean_rel - GAMMA * mean_nrel\n",
    "        \n",
    "        query_vecs[q_name] = query_vecs[q_name] / np.linalg.norm(query_vecs[q_name])\n",
    "\n",
    "        \n",
    "#     print(BETA * mean_rel)\n",
    "#     print(GAMMA * mean_nrel)\n",
    "#     print(tf_q)\n",
    "    # Update Rank\n",
    "    VSM_rank = ranking(query_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(sorted_sim_dict, filename = \"result.txt\"):\n",
    "    # output file\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "\n",
    "    with open(filename, \"w\") as ofile:\n",
    "        ofile.write(\"Query,RetrievedDocuments\\n\")\n",
    "        for query_name, score_list in sorted_sim_dict.items():\n",
    "            ofile.write(query_name + \",\")\n",
    "            for score in score_list:\n",
    "                ofile.write(score + \" \")\n",
    "            ofile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output(VSM_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "746.7830352783203"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking_BM(tf_q):\n",
    "    avgDL = avgDocLength(docs)\n",
    "\n",
    "    sim_dict = {}\n",
    "    sorted_sim_dict = {}\n",
    "    \n",
    "    i = 0\n",
    "    for fqname, terms_q in querys.items():\n",
    "        print(i)\n",
    "        i += 1\n",
    "        sim_dict[fqname] = {}\n",
    "        for fdname, terms_d in docs.items():\n",
    "            score = 0.0\n",
    "            # BM25\n",
    "            for term in terms_q:\n",
    "                if term not in terms_d:\n",
    "                    continue\n",
    "                term_id = word_2_id[term]\n",
    "                score += (k1 + 1) * tf[fdname][term_id] / (k1 * ((1 - b) + b * len(terms_d) / avgDL) + tf[fdname][term_id]) * idf[term_id]\n",
    "#                 score += (k1 + 1) * tf_d[fdname][term_id] / (k1 * ((1 - b) + b * len(terms_d) / avgDL) + tf_d[fdname][term_id]) \\\n",
    "#                              * idf[term_id] * (k3 + 1) * tf_q[fqname][term_id] / (k3 + tf_q[fqname][term_id])\n",
    "            sim_dict[fqname][fdname] = score\n",
    "        # Sort the sim score\n",
    "        sorted_sim_dict[fqname] = sorted(sim_dict[fqname], key=sim_dict[fqname].get, reverse=True)\n",
    "    return sorted_sim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking_VSM(query_vecs):\n",
    "    sim_dict = {}\n",
    "    sorted_sim_dict = {}\n",
    "    i = 0\n",
    "\n",
    "    for fqname, terms_q in querys.items():\n",
    "        print(i)\n",
    "        i += 1\n",
    "        sim_dict[fqname] = {}\n",
    "#         query_vec = []\n",
    "#         # query_vec\n",
    "#         for term in terms_q:\n",
    "#             term_id = word_2_id[term]\n",
    "#             try:\n",
    "#                 query_vec.append(query_vecs[fqname][term_id])\n",
    "#             except:\n",
    "#                 query_vec.append(0)\n",
    "        for fdname, terms_d in docs.items():\n",
    "#             doc_vec = []\n",
    "#             # VSM\n",
    "#             for term in terms_q:\n",
    "#                 term_id = word_2_id[term]\n",
    "#                 # doc_vec\n",
    "#                 try:\n",
    "#                     doc_vec.append(tfidf[fdname][term_id])\n",
    "#                 except:\n",
    "#                     dec_vec.append(0)\n",
    "            sim_dict[fqname][fdname] = cosine(query_vecs[fqname], tfidf[fdname])\n",
    "        sorted_sim_dict[fqname] = sorted(sim_dict[fqname], key=sim_dict[fqname].get, reverse=True)\n",
    "\n",
    "    return sorted_sim_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IR",
   "language": "python",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
